---
title: "Globant technical test"
author: "Jeison Mesa"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: pdf_document
bibliography: reference.bib
biblio-style: apalike
header-includes:
   - \usepackage[utf8]{inputenc}
   - \usepackage{float}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# set.seed(123)
```

```{r, load_data, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE}
library(readr)
library(dplyr)
library(ggplot2)
library(glue)
library(scales)
library(forcats)
library(naniar)
library(stringr)
library(knitr)
library(kableExtra)

clean_data <-  read_csv(file = glue::glue("data/cleaning/sequence_purchase_transactions.csv"))

```

## Introduction

In the workflow for data science professionals, it is important to achieve process automation. Automating processes allows to control human errors made by performing tasks manually. It is proposed to download files using web scraping. In this case, R is connected to the information available for each of the data sets in the web page. It is to recognise the structure of the information. It was possible to identify two types of data structure xls and csv files, for the set of csv files a totally different pattern was found to the xls format files, for this situation only the xls format files were keep.
The next step is to have a logical file name. In this case, having files for different months, it is proposed to work with the following format "PCT_year_month" for all the files. This is because the structure presented on the website seemed rather "messy".

The additional step is done to identify the columns that are found for all the files. Resulting in the following variables: 

```{r, see_variables, cache=TRUE}
colnames(clean_data)
```

As can be seen, a date column has been generated in ymd format; in addition to this, year, month and day columns were generated separately in order to have a more universal date format, it is also proposed to calculate the day of the week. In economics, it is usual to find different relationships for weekdays compared to weekends in financial variables. 

## Quality Control

The inferences that can be by a statistical model are largely influenced by the quality of the information, i.e. we must be able to guarantee that the information was verified and validated; for this purpose the following is generally checked:

1. __Problems associated with the scale of the variable__ 

Regardless of the type of software that we want to use, it is necessary that at the moment of loading the information those variables that by their typology are strictly numerical, the software can identify it because if we adjust a statistical model with a variable that is numerical and the software detects it as categorical we will reach erroneous conclusions (this is only an example of the many cases that can occur) or vice versa when giving numerical values to categorical variables the software detects them as numerical, in this case it is necessary to perform a transformation.

2. __Missing values__ 

It is important to be able to check which variables have a high percentage of missing values, since many of the optimisations of statistical models require having the complete data matrix. In this sense, if the information provided presents missing values for some of the variables, a process of imputation of values must be carried out (by some statistical technique ML, pca + NIPALS, KNN, etc...). But it is important to recognize the adjacent stochastic process that arises from credit card transactions. In essence, it is possible to consider a time series with irregular observations over time [@eyheramendy_elorrieta_palma_2016]. This is a new approach and can be analyzed by kalman filters and state space representation. In my master thesis I am proposing a regression approach with autocorrelated errors and irregular observations over time, such as credit transactions.

```{r, missing, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Valores Faltantes", out.width='50%', fig.align="center"}
gg_miss_var(clean_data, show_pct = T) + 
  labs(y = "% Missing Values") +
  ggtitle(label = "")
```


Assuming that traditional methods will be used, the greatest concentration of missing data is found in the "TRANS VAT DESC" variable (Figure \ref{fig:missing}), and for this reason it will not be used in the statistical inferences to be developed. 
But it is important to recognize the adjacent stochastic process that arises from credit card transactions. In essence, one can consider a time series with irregular observations over time. 

3. __Problems due to the Characteristic of your Variables__.

In many cases we cannot assume that the information is correct, since there may be typing problems or problems due to "outliers" (e.g. imagine a negative precipitation is not possible). When the problem related to the outlier is trivial, i.e. we can know with certainty that it is a value that cannot occur, we can choose to induce a missing value for that data. On the other hand, in order to identify possible outliers it is necessary to use descriptive or inferential statistical techniques; for the descriptive option a box plot can be performed and for the inferential case a probability distribution can be fitted and those values which are not under the distribution curve can be determined. 

```{r, boxplot, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Gross Outlier Detection", out.width='50%', fig.align="center"}

ggplot()+
  geom_boxplot(data = clean_data, aes(x = "", y = `ORIGINAL GROSS AMT`))+
  theme_bw()+
  labs(y = "Original Gross AMT £")

```


The Figure \ref{fig:boxplot} shows possible outliers for some transactions. In this case we have a record that may be due to a return of around £5000000 and a value per purchase of more than £250000. These are points that we will later evaluate if they can truly be considered as outliers or fraud issues. 

Why is it necessary to develop a deeper analysis? It is necessary because we cannot consider outliers under the assumption of the interquartile ranges of the boxplot. Other types of outlier distributions such as the Gumbel distribution or non-parametric methods can be considered.

```{r, time_gross, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Time Serie Gross AMT", out.width='50%', fig.align="center"}

imputeTS::ggplot_na_distribution(
  x = clean_data$`ORIGINAL GROSS AMT`, 
  x_axis_labels = clean_data$date,
  size_points = 1
) +
  theme_bw()+
  labs(y = "Original Gross AMT £", x = "Date")

```

The figure \ref{fig:time_gross} shows the dynamics associated with the transactions carried out. In particular, some rather "strange" points can be observed according to the records. The red areas indicate two cases: no transactions were made or no information was found for those periods. 

Now it is necessary to perform descriptive statistics that provide valuable information when generating a statistical model. 

The most relevant results are shown below.


```{r, gross_merchan, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Activity in credit card transaction (percentages are the proportion of the number of times the card was used for the particular merchant, the bars indicate the amount of money used)", out.width='75%', fig.align="center"}

prop_merchant_name <- clean_data %>% 
  count(`MERCHANT NAME`) %>% 
  filter(!is.na(`MERCHANT NAME`)) %>% 
  mutate(prop = n/sum(n)*100) %>% 
  arrange(desc(n))

merchant_name_sum <- clean_data %>% 
  group_by(`MERCHANT NAME`) %>% 
  summarise(sum_gross_amt = sum(`ORIGINAL GROSS AMT`, na.rm = T)) %>% 
  arrange(desc(sum_gross_amt)) %>% 
  left_join(prop_merchant_name, by = "MERCHANT NAME") %>% 
  top_n(n = 20, wt = sum_gross_amt) %>% 
  mutate(prop = prop/100)

ggplot()+
  geom_col(data = merchant_name_sum, aes(x = fct_reorder(`MERCHANT NAME`,
                                                         sum_gross_amt, .desc = F), y = sum_gross_amt))+
  coord_flip()+
  geom_text(data = merchant_name_sum, aes(x = fct_reorder(`MERCHANT NAME`,
                                                          sum_gross_amt, .desc = F),
                                          label = scales::percent(round(prop, 3)),
                 y= sum_gross_amt), vjust = .5, hjust = -0.1)+
  # ggplot2::lims(y = c(0, 800000))+
  scale_y_continuous(labels = comma, limits = c(0, 8000000))+
  theme_bw()+
  labs(y = "Original Gross AMT £", x = "MERCHANT NAME")

```

The Figure \ref{fig:gross_merchan} shows that the client for which the most money was spent was "the furnishing service" but not as many transactions were recorded as in the case of "travelodge gb0000". It is also possible to look at several transactions for amazon but different divisions within amazon, now we can aggregate for the amazon category (additionally if we have a team it is possible to collect other variables).

```{r, gross_merchan_amazon, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Activity in credit card transaction (percentages are the proportion of the number of times the card was used for the particular merchant, the bars indicate the amount of money used) (Amazon)", out.width='75%', fig.align="center"}

prop_merchant_name <- clean_data %>% 
  mutate(condition = str_detect(`MERCHANT NAME`, fixed('amazon', ignore_case=TRUE))) %>% 
  mutate(`MERCHANT NAME` = if_else(condition == TRUE, "amazon", `MERCHANT NAME`)) %>% 
  count(`MERCHANT NAME`) %>% 
  filter(!is.na(`MERCHANT NAME`)) %>% 
  mutate(prop = n/sum(n)*100) %>% 
  arrange(desc(n))

merchant_amazon <- clean_data %>% 
  mutate(condition = str_detect(`MERCHANT NAME`, fixed('amazon', ignore_case=TRUE))) %>% 
  mutate(`MERCHANT NAME` = if_else(condition == TRUE, "amazon", `MERCHANT NAME`)) %>% 
  group_by(`MERCHANT NAME`) %>% 
  summarise(sum_gross_amt = sum(`ORIGINAL GROSS AMT`, na.rm = T)) %>% 
  arrange(desc(sum_gross_amt)) %>% 
  left_join(prop_merchant_name, by = "MERCHANT NAME") %>% 
  top_n(n = 20, wt = sum_gross_amt) %>% 
  mutate(prop = prop/100)

ggplot()+
  geom_col(data = merchant_amazon, aes(x = fct_reorder(`MERCHANT NAME`,
                                                         sum_gross_amt, .desc = F), y = sum_gross_amt))+
  coord_flip()+
  geom_text(data = merchant_amazon, aes(x = fct_reorder(`MERCHANT NAME`,
                                                          sum_gross_amt, .desc = F),
                                          label = scales::percent(round(prop, 3)),
                                          y= sum_gross_amt), vjust = .5, hjust = -0.1)+
  # ggplot2::lims(y = c(0, 800000))+
  scale_y_continuous(labels = comma, limits = c(0, 8000000))+
  theme_bw()+
  labs(y = "Original Gross AMT £", x = "MERCHANT NAME") +
  ggtitle("With all amazon's divisions in one")

```

The Figure \ref{fig:gross_merchan_amazon} shows that amazon moved up in the ranking, but the associated spend per transaction is still higher for the "the furnishing service" category. This analysis can be extended to other variables. 

```{r, trans_dec_1, warning=FALSE, cache=TRUE, eval=TRUE, message =FALSE, fig.align='center',  echo=FALSE}

clean_data %>% 
  count(`TRANS CAC DESC 1`) %>% 
  filter(!is.na(`TRANS CAC DESC 1`)) %>% 
  mutate(prop = round(n/sum(n)*100, 2)) %>%
  mutate(prop = glue("{prop}%")) %>% 
  arrange(desc(n)) %>% 
  top_n(n = 10, wt =  n) %>% 
  kable(caption = 'Sector top 10', format="latex") %>%
  kable_styling(latex_options = "hold_position")
```

The Table \ref{tab:trans_dec_1} shows that the sector with the highest transaction is "Equip Operational". In addition, another important use of the card is for "Vehicle Fuel" and "Purchases Food", which represent a person's daily expenses. 

Other important descriptive statistics are included

```{r, trans_var, warning=FALSE, cache=TRUE, eval=TRUE, message =FALSE, fig.align='center',  echo=FALSE}

clean_data %>% 
  count(`TRANS VAT DESC`) %>% 
  filter(!is.na(`TRANS VAT DESC`)) %>% 
  mutate(prop = round(n/sum(n)*100, 2)) %>% 
  arrange(desc(n)) %>%
  mutate(prop = glue("{prop}%")) %>%  
  kable(caption = 'TRANS VAT DESC top 10', format="latex") %>%
  kable_styling(latex_options = "hold_position")
```


```{r, trans_cac_2, warning=FALSE, cache=TRUE, eval=TRUE, message =FALSE, fig.align='center',  echo=FALSE}

clean_data %>% 
  count(`TRANS CAC DESC 2`) %>% 
  filter(!is.na(`TRANS CAC DESC 2`)) %>% 
  mutate(prop = round(n/sum(n)*100, 2)) %>% 
  arrange(desc(n)) %>% 
  top_n(n = 10, wt =  n) %>%
  mutate(prop = glue("{prop}%")) %>% 
  kable(caption = 'TRANS CAC DESC 2 top 10', format="latex") %>%
  kable_styling(latex_options = "hold_position")
```


```{r, wday, warning=FALSE, cache=TRUE, eval=TRUE, message =FALSE, fig.align='center',  echo=FALSE}

clean_data %>% 
  count(wday) %>% 
  filter(!is.na(wday)) %>% 
  mutate(prop = round(n/sum(n)*100, 2)) %>% 
  arrange(desc(n)) %>% 
  top_n(n = 10, wt =  n) %>%
  mutate(prop = glue("{prop}%")) %>% 
  kable(caption = 'Day of the week for the transaction', format="latex") %>%
  kable_styling(latex_options = "hold_position")
```

The Table \ref{tab:wday} shows something very important, the transactions are being made on weekdays, since there is little use on weekends. This may indicate the fact that the most of the transactions are for "Equip Operational" as these are being made on weekdays.

## Model Analysis

In many data anlaysis tasks, outlier detection plays an important role in modelling, inference, and even data processing because otuliers could adversely lead to model misspecification, biased parameter estimation, and poor predictions. The original outlier detection methods were arbitrary like using boxplot analys (interquartile range). But recognizing the pattern of transactions, outlier analysis can be performed with probability distributions for extreme values such as the gumbel distribution.

```{r, model_data, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=T}
library(readr)
library(glue)
library(FactoMineR)
library(tidyr)
library(factoextra)
library(sparklyr)
library(dplyr)
library(caret)
library(tidymodels)
library(C50)
library(parsnip)
library(ranger)
library(ggpubr)
library(skimr)
library(extRemes)


clean_data <-  read_csv(file = glue::glue("data/cleaning/sequence_purchase_transactions.csv")) %>% 
  mutate(week_type = case_when(wday == "Sat" ~ "weekend",
                               wday == "Sun" ~ "weekend",
                               TRUE ~ "Week")) %>% 
  dplyr::select(-wday)
names(clean_data) <- gsub(" ", "_", names(clean_data))

clean_data <- clean_data %>% 
  mutate(type = case_when(ORIGINAL_GROSS_AMT >0 ~ "purchase",
                          ORIGINAL_GROSS_AMT <=0 ~ "return"))


# Hampel filter

lower_bound <- median(clean_data$ORIGINAL_GROSS_AMT, na.rm = T) - 3 * mad(clean_data$ORIGINAL_GROSS_AMT, 
                                                                          constant = 1, na.rm = T)

upper_bound <- median(clean_data$ORIGINAL_GROSS_AMT, na.rm = T) + 3 * mad(clean_data$ORIGINAL_GROSS_AMT, 
                                                                          constant = 1, na.rm = T)
                                                                          
                                                                          
complete_data <- clean_data %>% 
  mutate(condition = case_when(ORIGINAL_GROSS_AMT <= lower_bound ~ "outlier", 
                               ORIGINAL_GROSS_AMT >= upper_bound ~ "outlier",
                               TRUE ~ "normal")) %>% 
  drop_na()

complete_data %>% 
  filter(row_number()==1 | row_number()==n()) %>% 
  dplyr::select(date)

```

```{r, gumbel, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "Gumbel", out.width='75%', fig.align="center"}

fit <- fevd(complete_data$ORIGINAL_GROSS_AMT, type = "Gumbel")
plot(fit, type = "density", main = "Empirical density vs estimated Gumbel distribution")
```

Values are concentrated around low amounts (Figure \ref{fig:gumbel}). Therefore, large amounts of money used in transactions can be considered as fraud problems. Additionally, it is possible to use the "Hampel filter" metric to determine the bands in which possible outliers are found (this metric is useful as it is a non-parametric method) and onsists of considering as outliers the values outside the interval $(I)$ formed by the median, plus or minus 3 median absolute deviations.


```{r, Hampel, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=T}
lower_bound
upper_bound 
   
```

Values below `r lower_bound` and values above `r upper_bound` are considered "outliers" which represent information different from the information expected to be found. For now they will be considered as possible outliers, but we will adjust decision trees model to contrast the fitted values of the model with the possible leverage points of the line. In regression analysis, leverage points cause poor modeling. In this sense those leverage points will be considered to be outside the expected value of the distribution.

__Description__

The term machine learning encompasses the set of algorithms that identify patterns in data and create structures (models) that represent them. Once the models have been generated, they can be used to predict information about facts or events that have not yet been observed. It is important to remember that machine learning systems are only able to memorize patterns that are present in the data they are trained on, so they can only recognize what they have seen before. By using systems trained with past data to predict the future, it is assumed that the behavior will be the same in the future, which is not always the case.

__Modelling Stages__

1. Preparring the strategy for evaluating the model: separate the observations into a training set, a validation (or cross-validation) set and a test set. It is very important to ensure that no information from the test set is involved in the model training process.

2. Preprocessing the data: apply the necessary transformations so that the data can be interpreted by the selected machine learning algorithm.

3. Adjust a first model capable of overcoming minimum results.

4. Gradually improve the model by incorporating-creating new variables or optimizing the hyperparameters.

5. Evaluating the capacity of the final model with the test set to have an estimate of the capacity of the model when predicting new observations.

6. Train the final model with all available data.

The [Tidymodels](https://www.tidymodels.org) environment will be used for modeling development. Tidymodels is an interface that unifies under a single framework hundreds of functions from different packages, greatly facilitating all stages of preprocessing, training, optimization and validation of predictive models.

```{r, model_tree, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=T}

set.seed(1234)

split_inicial <- initial_split(
  data   = complete_data,
  prop   = 0.8,
  strata = ORIGINAL_GROSS_AMT
)
datos_train <- training(split_inicial)
datos_test  <- testing(split_inicial)

transformer <- recipe(
  formula = ORIGINAL_GROSS_AMT ~ TRANS_VAT_DESC+TRANS_CAC_CODE_3+TRANS_CAC_CODE_1+week_type+condition,
  data =  datos_train
) %>%
  step_naomit(all_predictors()) %>%
  step_nzv(all_predictors()) %>%
  step_center(all_numeric(), -all_outcomes()) %>%
  step_scale(all_numeric(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes())


transformer_fit <- prep(transformer)

# The transformations are applied to the training and test set.

datos_train_prep <- bake(transformer_fit, new_data = datos_train)
datos_test_prep  <- bake(transformer_fit, new_data = datos_test)

glimpse(datos_train_prep)

modelo_tree <- decision_tree(mode = "regression") %>%
  set_engine(engine = "rpart")
modelo_tree

response_variable <- "ORIGINAL_GROSS_AMT"

predictor_variables <- setdiff(colnames(datos_train_prep), response_variable)

modelo_tree_fit <- modelo_tree %>%
  fit_xy(
    x = datos_train_prep[, predictor_variables],
    y = datos_train_prep[[response_variable]]
  )

modelo_tree_fit$fit

cv_folds <- vfold_cv(
  data    = datos_train,
  v       = 5,
  repeats = 10,
  strata  = ORIGINAL_GROSS_AMT
)
head(cv_folds)

modelo_tree <- decision_tree(mode = "regression") %>%
  set_engine(engine = "rpart")

validacion_fit <- fit_resamples(
  object       = modelo_tree,
  preprocessor = transformer,
  resamples    = cv_folds,
  metrics      = metric_set(rmse, mae),
  control      = control_resamples(save_pred = TRUE)
)

head(validacion_fit)

validacion_fit %>% 
  collect_metrics(summarize = TRUE)

validacion_fit %>%
  collect_metrics(summarize = FALSE) %>% head()


```


```{r, validacion_graph, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "", out.width='77%', fig.align="center"}

p1 <- ggplot(
  data = validacion_fit %>% collect_metrics(summarize = FALSE),
  aes(x = .estimate, fill = .metric)) +
  geom_density(alpha = 0.5) +
  theme_bw() 

p2 <- ggplot(
  data = validacion_fit %>% collect_metrics(summarize = FALSE),
  aes(x = .metric, y = .estimate, fill = .metric, color = .metric)) +
  geom_boxplot(outlier.shape = NA, alpha = 0.1) +
  geom_jitter(width = 0.05, alpha = 0.3) +
  coord_flip() +
  theme_bw() +
  theme(axis.text.x = element_blank(), axis.ticks.x = element_blank())

ggarrange(p1, p2, nrow = 2, common.legend = TRUE, align = "v") %>% 
  annotate_figure(
    top = text_grob("Errors Distribution k-fold", size = 15)
  )
```

```{r, fitted_graph, cache=TRUE, eval=TRUE, message =FALSE, warning=FALSE, warning=FALSE, echo=FALSE, fig.cap = "fitted vs Response", out.width='77%', fig.align="center"}

ajustados <- validacion_fit %>% collect_predictions(summarize = TRUE) 

ggplot(
  data = validacion_fit %>% collect_predictions(summarize = TRUE),
  aes(x = ORIGINAL_GROSS_AMT, y = .pred)
) +
  geom_point(alpha = 0.3) +
  geom_abline(slope = 1, intercept = 0, color = "firebrick") +
  geom_vline(xintercept = 90000)
  labs(title = "") +
  theme_bw()+
  scale_y_continuous(labels = comma)
```
  
It is possible to perform an analysis of extreme events as shown in the Figure \ref{fig:fitted_graph}, given that there are tre large concentrations of the amount of money spent per transaction, amounts below £1000 and amounts around £3000 and amounts around £90000. This implies that transactions above £90000 are unusual considering the whole history of transactions.

## References